{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizer_xm import *\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = pd.read_csv(\"data/unqiue_bb_data.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_noun_adj:\n",
    "    \"\"\"\n",
    "    Get the nouns and Adjectives used in a collection of text. \n",
    "    \n",
    "    ---return\n",
    "    Returns a class that contains:\n",
    "    1. A pandas dataframe with all unique nouns with DF and TF\n",
    "    2. A pandas dataframe with all unique adjectives with DF and TF\n",
    "    \"\"\"\n",
    "    def __init__(self, text, stopwords = []):\n",
    "        self.text =text\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    \n",
    "    def flatten(self,listOfLists):\n",
    "        from itertools import chain\n",
    "        \"Flatten one level of nesting\"\n",
    "        return list(chain.from_iterable(listOfLists))\n",
    "    \n",
    "    def get_Nouns_Adjs(self, show_top_n = 15):\n",
    "        \n",
    "        \"\"\"\n",
    "        ---Parameter\n",
    "        show_top_n: int, Show top n popular adjs and nouns        \n",
    "        \"\"\"\n",
    "        \n",
    "        # simple_process the documents\n",
    "        simple_processed_doc = text_tokenizer_xm(text = self.text, lemma_flag = True,stem_flag = False,\\\n",
    "                                  stopwords=[]).txt_pre_pros_all()\n",
    "                \n",
    "        # Tag each word within a list of tokenized documents\n",
    "        tagged_doc = pd.Series(simple_processed_doc).apply(lambda x: pos_tag(x))\n",
    "\n",
    "        # Extract all the tags\n",
    "        tags = tagged_doc.apply(lambda x: [tup[1] for tup in x])\n",
    "        \n",
    "        # Remove the stopwords\n",
    "        tagged_doc_cleaned = tagged_doc.apply(lambda x: [tup for tup in x if tup[0] not in self.stopwords])\n",
    "        \n",
    "        # Create a separate list where each term + tag combination only appears once in each tagged document.\n",
    "        # This list is to calculate document frequency\n",
    "        tagged_doc_unique = [list(set(x)) for x in tagged_doc_cleaned]\n",
    "                \n",
    "        # Get all the tuples\n",
    "        all_tups = self.flatten(tagged_doc_unique)\n",
    "        \n",
    "        # Get all the terms for term-frequency\n",
    "        all_tups_tf = self.flatten(tagged_doc_cleaned)\n",
    "        all_tups_term = np.array([tup[0] for tup in all_tups_tf])        \n",
    "\n",
    "        \n",
    "        # Find all the Nouns\n",
    "        idx_n = [(tup[1] in ['NN','NNS','NNP','NNPS']) for tup in all_tups]\n",
    "        all_nouns_raw = (pd.Series([tup[0] for tup in all_tups])[idx_n])\n",
    "        # Lemmatize to merge terms like rashes, babies to rash, baby.\n",
    "        all_nouns = self.flatten(list(text_tokenizer_xm(text = all_nouns_raw,stem_flag=False, \\\n",
    "                                                         lemma_flag=True,stopwords=[]).txt_pre_pros_all()))\n",
    "\n",
    "        # Find all the Adjs\n",
    "        idx_a = [(tup[1] in ['JJ','JJS','JJR']) for tup in all_tups]\n",
    "        all_adjs_raw = (pd.Series([tup[0] for tup in all_tups])[idx_a])\n",
    "        all_adjs = self.flatten(list(text_tokenizer_xm(text = all_adjs_raw,stem_flag=False, \\\n",
    "                                                         lemma_flag=True,stopwords=[]).txt_pre_pros_all()))\n",
    "        # Construct the Noun Table\n",
    "        all_noun = pd.DataFrame({'Nouns':all_nouns})\n",
    "        all_noun_agg = pd.DataFrame(all_noun.groupby('Nouns').size().\\\n",
    "                                    sort_values(ascending = False)).reset_index().head(show_top_n)\n",
    "        all_noun_agg.columns = ['Terms','Document_Frequency']\n",
    "\n",
    "        # Constuct the Adj Table\n",
    "        all_adj = pd.DataFrame({'Adjs':all_adjs})\n",
    "        all_adj_agg = pd.DataFrame(all_adj.groupby('Adjs').size().\\\n",
    "                                   sort_values(ascending = False)).reset_index().head(show_top_n)\n",
    "        all_adj_agg.columns = ['Terms','Document_Frequency']\n",
    "        \n",
    "        # Here, the document frequency is not accurate. Some tokens, like \"product\" can be tagged as \n",
    "        # NN or NNS in the simple preprocess documents where we don't lemmatize. Therefore, the term will\n",
    "        # Get over counted in the document frequency calculation above. Unless we lemmatize the term in\n",
    "        # simple preprocess\n",
    "\n",
    "        class output:\n",
    "            all_adj  = all_adj_agg\n",
    "            all_noun = all_noun_agg\n",
    "            tagged_terms = tagged_doc_cleaned            \n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting knowledge:https://stackoverflow.com/questions/23944657/typeerror-method-takes-1-positional-argument-but-2-were-given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(bb['review_text'].astype(str).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = get_noun_adj(text = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = na.get_Nouns_Adjs(show_top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how about, simply collect the terms and the processed documents. Then utilize keras to get the tfidf, tf and df scores?\n",
    "\n",
    "This doesn't work. The process is way too tedious just for a tfidf calculation.\n",
    "\n",
    "Also, since keras' tfidf vectorizer is implemented separately, it seems sometimes the two program will have \"disagreements.\" e.g. we found top 20 adjs with the original program, but some of the 20 adjs is not found by the keras tfidf vectorizer, which could cause huge confusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful link to help recall the definitions of TFIDF.:\n",
    "http://www.tfidf.com/\n",
    "\n",
    "An interesting note is: Term-frequency is not the same as the count of Terms. Term-Frequency, in the TFIDF setting, is the number of times a term appears **in each document**, and it is often weighted by the total number of terms in the document. Term count, which is merely the total number of occurance of a term, can be understood as the sum of TF (not weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a trial I did to add an accurate find_match and idf, tf, df calculation. But it significantly slows down the calculation and the find_match function seem to be very vulnerable to the quality of text. Thus I removed the chunck and save it incase we need to use them in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df = []\n",
    "adj_df_idx = []\n",
    "adj_term_count = []\n",
    "n_df = []\n",
    "n_df_idx = []\n",
    "n_term_count = []\n",
    "\n",
    "# Loop through the adjs\n",
    "for term in all_adj_agg['Terms']:\n",
    "    tf = np.array([find_match(term,document,lemma = True, stem = False,unigram_return_count = True)\\\n",
    "                   for document in self.text])\n",
    "    # Calcualte the Document Frequency and Inverse Document Frequency\n",
    "    df = sum(tf != 0)\n",
    "    adj_df.append(df)\n",
    "    adj_term_count.append(sum(tf))\n",
    "    adj_df_idx.append(list(tf != 0))\n",
    "\n",
    "# Loop through the nouns\n",
    "for term in all_noun_agg['Terms']:\n",
    "    tf = np.array([find_match(term,document,lemma = True, stem = False,unigram_return_count = True)\\\n",
    "                   for document in self.text])\n",
    "    # Calcualte the Document Frequency and Inverse Document Frequency\n",
    "    df = sum(tf != 0)\n",
    "    n_df.append(df)\n",
    "    n_term_count.append(sum(tf))\n",
    "    n_df_idx.append(list(tf != 0))\n",
    "\n",
    "# Create columns\n",
    "all_noun_agg['Document_Frequency'] = n_df\n",
    "all_noun_agg['Total_Occurance'] = n_term_count\n",
    "\n",
    "all_adj_agg['Document_Frequency'] = adj_df\n",
    "all_adj_agg['Total_Occurance'] = adj_term_count\n",
    "\n",
    "self.adj_df_idx = adj_df_idx\n",
    "self.n_df_idx = n_df_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
